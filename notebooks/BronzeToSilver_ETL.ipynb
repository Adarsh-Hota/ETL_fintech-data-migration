{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "# Omitting the container and storage account name for security reasons\n",
    "base_path = \"abfss://container_name@storage_account_name.dfs.core.windows.net/bronze/fintech/\"\n",
    "output_base_path = \"abfss://container_name@storage_account_name.dfs.core.windows.net/silver/fintech/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Transformation for Accounts dataset - \n",
    "    A new column, \"AccountAgeYears\", is added, which calculates how long each account has been active \n",
    "    by determining the difference between the current date and the account's open date, then converting it to years.\n",
    "'''\n",
    "def transform_accounts():\n",
    "    df = spark.read.parquet(f\"{base_path}Accounts/Accounts.parquet\")\n",
    "    # Transformation: Calculate account age in years\n",
    "    df_transformed = df.withColumn(\"AccountAgeYears\", \n",
    "                                   round(datediff(current_date(), col(\"OpenDate\")) / 365.25, 2))\n",
    "    df_transformed.write.format(\"delta\").mode(\"overwrite\").save(f\"{output_base_path}Accounts/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Transformation for Customers dataset - \n",
    "    A new FullName column is created by concatenating FirstName and LastName.\n",
    "    A MaskedEmail column is created, where the email address is partially hidden \n",
    "    (the part before the @ symbol is replaced by ***).\n",
    "'''\n",
    "def transform_customers():\n",
    "    df = spark.read.parquet(f\"{base_path}Customers/Customers.parquet\")\n",
    "    # Transformation: Create a full name column and mask the email address\n",
    "    df_transformed = df.withColumn(\"FullName\", concat_ws(\" \", col(\"FirstName\"), col(\"LastName\"))) \\\n",
    "                       .withColumn(\"MaskedEmail\", \n",
    "                                   concat(lit(\"***@\"), substring_index(col(\"Email\"), \"@\", -1)))\n",
    "    df_transformed.write.format(\"delta\").mode(\"overwrite\").save(f\"{output_base_path}Customers/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Transformation for Loans dataset - \n",
    "    TotalInterest: The total interest paid on the loan.\n",
    "    LoanDurationYears: The duration of the loan in years.\n",
    "\n",
    "Total Interest Calculation:\n",
    "    The code calculates the total interest on the loan using the formula:\n",
    "            TotalInterest = (LoanAmount X InterestRate)/100 \n",
    "    The result is explicitly cast to decimal(28,8) to ensure precise storage in the Delta table.\n",
    "\n",
    "Loan Duration Calculation:\n",
    "    The duration of the loan in years is calculated based on the difference between the LoanEndDate and LoanStartDate.\n",
    "    The result is rounded to 2 decimal places for clarity.\n",
    "'''\n",
    "def transform_loans():\n",
    "    df = spark.read.parquet(f\"{base_path}Loans/Loans.parquet\")\n",
    "    # Transformation: Calculate total interest with explicit casting to match the Delta table\n",
    "    df_transformed = df.withColumn(\"TotalInterest\", \n",
    "                                   (col(\"LoanAmount\") * col(\"InterestRate\") / 100).cast(\"decimal(28,8)\")) \\\n",
    "                       .withColumn(\"LoanDurationYears\", \n",
    "                                   round(datediff(col(\"LoanEndDate\"), col(\"LoanStartDate\")) / 365.25, 2))\n",
    "    df_transformed.write.format(\"delta\").mode(\"overwrite\").save(f\"{output_base_path}Loans/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Transformation for Payments dataset - \n",
    "    Transform the dataset by adding a new column DaysSinceLastPayment, \n",
    "    which calculates how many days have passed since the last payment.\n",
    "'''\n",
    "def transform_payments():\n",
    "    df = spark.read.parquet(f\"{base_path}Payments/Payments.parquet\")\n",
    "    # Transformation: Calculate days since last payment\n",
    "    df_transformed = df.withColumn(\"DaysSinceLastPayment\", \n",
    "                                   datediff(current_date(), col(\"PaymentDate\")))\n",
    "    df_transformed.write.format(\"delta\").mode(\"overwrite\").save(f\"{output_base_path}Payments/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Transformation for Transactions dataset - \n",
    "    Transform the dataset by creating a new column TransactionCategory based on the value of the TransactionType column.\n",
    "    If the transaction is a Deposit, it's categorized as Income.\n",
    "    If it's a Withdrawal, it's categorized as Expense.\n",
    "    Any other transaction type falls into the Other category.\n",
    "'''\n",
    "def transform_transactions():\n",
    "    df = spark.read.parquet(f\"{base_path}Transactions/Transactions.parquet\")\n",
    "    # Transformation: Categorize transaction types\n",
    "    df_transformed = df.withColumn(\"TransactionCategory\", \n",
    "                                   when(col(\"TransactionType\") == \"Deposit\", \"Income\")\n",
    "                                   .when(col(\"TransactionType\") == \"Withdrawal\", \"Expense\")\n",
    "                                   .otherwise(\"Other\"))\n",
    "    df_transformed.write.format(\"delta\").mode(\"overwrite\").save(f\"{output_base_path}Transactions/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Process each table - \n",
    "    transform_accounts(): Processes the Accounts dataset, typically applying transformations \n",
    "    like calculating account age.\n",
    "\n",
    "    transform_customers(): Processes the Customers dataset, applying transformations such as \n",
    "    creating a full name and masking email addresses.\n",
    "\n",
    "    transform_loans(): Processes the Loans dataset, calculating fields like total interest and loan duration.\n",
    "\n",
    "    transform_payments(): Processes the Payments dataset, calculating how many days have passed\n",
    "    since the last payment.\n",
    "\n",
    "    transform_transactions(): Processes the Transactions dataset, categorizing transaction types \n",
    "    (e.g., Deposit -> Income, Withdrawal -> Expense).\n",
    "'''\n",
    "transform_accounts()\n",
    "transform_customers()\n",
    "transform_loans()\n",
    "transform_payments()\n",
    "transform_transactions()\n",
    "\n",
    "print(\"Bronze To Silver Completed !!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "save_output": true,
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
